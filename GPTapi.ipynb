{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015b9090-a37e-4a57-ba1b-6f3408112707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa7d5510-e5e1-497d-b351-2311e4c053b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root_folder = \"/home/ivan/Helmholtz/Conferences/APIworkshopHelmholtz2025/\"\n",
    "dataset_folder = os.path.join(project_root_folder, \"AcevedoDataSet\")\n",
    "testset_folder =  os.path.join(dataset_folder, \"test\")\n",
    "trainset_folder = os.path.join(dataset_folder, \"train\")\n",
    "valset_folder = os.path.join(dataset_folder, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579a70b6-1b5b-4914-8fca-0dd5874d9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7238b87a-9260-4da0-a208-275552e9c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca3f39e-34a2-4fd0-9cbb-1e2ed6fbbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_api_text_inquiry(prompt_text, vlm_name='gpt-4o', **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=vlm_name, #\"o1-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_text\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer=response.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    # Extract and print token usage\n",
    "    usage = response.usage.total_tokens\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65595741-0236-4d60-b9b8-42cff8b53c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: If I were old school, a classic video game I might play is \"Pac-Man,\" which was released in 1980 and remains iconic in the gaming world. To beat the game, I could use a machine learning approach such as Reinforcement Learning (RL), specifically a Deep Q-Network (DQN).\n",
      "\n",
      "Reinforcement Learning is well-suited for this type of game because it focuses on learning optimal actions in an environment to maximize a cumulative reward. In the context of Pac-Man, the environment comprises the maze, dots, power-pellets, ghosts, and walls. The agent (Pac-Man) would learn strategies, like when and how to avoid ghosts or when to chase them after eating a power-pellet.\n",
      "\n",
      "A Deep Q-Network uses a neural network to approximate the Q-values (value of taking a certain action in a given state) and improves on traditional Q-learning by allowing the RL agent to make use of convoluted sensory input, such as screen pixels. Additionally, techniques like experience replay and target networks can stabilize and improve learning of the DQN, making it more effective in complex environments like Pac-Man.\n",
      "Tokens used: 261\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"If you were old school, which videogame would you play? What machine learning model would you use to beat the game?\"\n",
    "\n",
    "answer, usage = gpt_api_text_inquiry(prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38db4d31-9fc3-40fd-aea5-69296f241ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_api_visual_inquiry(image_path, prompt_text, vlm_name='gpt-4o', **kwargs):\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    # Set default detail if not provided in kwargs\n",
    "    detail = kwargs.get('detail', 'low')\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=vlm_name, #\"gpt-4o-mini\"\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": prompt_text,\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\":  f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                \"detail\": detail\n",
    "              },\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "    )\n",
    "    \n",
    "    answer=response.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    # Extract and print token usage\n",
    "    usage = response.usage.total_tokens\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f050547-98fe-44df-a14f-495accf811cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lymphocyte\n",
      "Tokens used: 217\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "prompt_text = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "answer, usage = gpt_api_visual_inquiry(image_path, prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee0ab333-8f4d-400b-8ef9-d63b5742abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_multiimage_api_visual_inquiry(image_paths, prompt_texts, vlm_name='gpt-4o', **kwargs):\n",
    "\n",
    "    if len(image_paths) != len(prompt_texts):\n",
    "        raise ValueError(\"The number of image paths and prompt texts must be the same.\")\n",
    "\n",
    "    def prepare_messages(prompt_texts, image_paths, detail=\"low\"):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": []\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for prompt_text, image_path in zip(prompt_texts, image_paths):\n",
    "            # Getting the base64 string\n",
    "            base64_image = encode_image(image_path)\n",
    "\n",
    "            messages[0][\"content\"].append({\"type\": \"text\", \"text\": prompt_text})\n",
    "            messages[0][\"content\"].append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\": detail\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    # Set default detail if not provided in kwargs\n",
    "    detail = kwargs.get('detail', 'low')\n",
    "\n",
    "    messages = prepare_messages(prompt_texts, image_paths, detail)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=vlm_name, #\"gpt-4o-mini\"\n",
    "      messages=messages\n",
    "    )\n",
    "    \n",
    "    answer=response.choices[0].message.content\n",
    "    \n",
    "    \n",
    "    # Extract and print token usage\n",
    "    usage = response.usage.total_tokens\n",
    "    \n",
    "    return answer, usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f29a0221-267a-47cb-8af3-c63da9cc9afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Platelet\n",
      "Tokens used: 500\n"
     ]
    }
   ],
   "source": [
    "# Train:\n",
    "train_path_0 = os.path.join(trainset_folder, \"image_0.jpg\") # It's a Myelocyte\n",
    "train_prompt_0 = \"The cell type in this image is Myelocyte\"\n",
    "\n",
    "train_path_1 = os.path.join(trainset_folder, \"image_2.jpg\") # It's a Platelet\n",
    "train_prompt_1 = \"The cell type in this image is Platelet\"\n",
    "\n",
    "train_path_2 = os.path.join(trainset_folder, \"image_9.jpg\") # It's a Lymphocyte\n",
    "train_prompt_2 = \"The cell type in this image is Lymphocyte\"\n",
    "\n",
    "# Test\n",
    "test_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "test_prompt = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "image_paths = [train_path_0, train_path_1, train_path_2, test_path]\n",
    "prompt_texts = [train_prompt_0, train_prompt_1, train_prompt_2, test_prompt]\n",
    "\n",
    "answer, usage = gpt_multiimage_api_visual_inquiry(image_paths, prompt_texts)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100688c6-ea59-4753-b7fe-ec390d372b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
