{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5e9f2b-0be9-444d-9ef0-1d037f9f0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/envs/vlm_eval/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af23c368-2f8d-4f9b-97da-5e6cca2e4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root_folder = \"/home/ivan/Helmholtz/Conferences/APIworkshopHelmholtz2025/\"\n",
    "dataset_folder = os.path.join(project_root_folder, \"AcevedoDataSet\")\n",
    "testset_folder =  os.path.join(dataset_folder, \"test\")\n",
    "trainset_folder = os.path.join(dataset_folder, \"train\")\n",
    "valset_folder = os.path.join(dataset_folder, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff88037b-aa73-4aad-85cc-98d158829143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_api_text_inquiry(prompt_text, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "    \n",
    "    response = model.generate_content(prompt_text)\n",
    "\n",
    "    answer = response.text\n",
    "\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc6b2a6-3b15-4041-a844-a5200057d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Okay, if I were going old school, I'd definitely choose **Space Invaders**. It's simple, iconic, and *deceptively* challenging.\n",
      "\n",
      "Now, for the machine learning model... I'd go with **Q-Learning**, likely with some enhancements to address the scaling issues that Space Invaders might present. Here's why, and how I'd approach it:\n",
      "\n",
      "**Why Q-Learning?**\n",
      "\n",
      "*   **Discrete Action Space:** Space Invaders has a relatively small and discrete action space:  move left, move right, shoot (or sometimes \"do nothing\").  Q-Learning thrives in environments with discrete actions.\n",
      "*   **Markov Decision Process (MDP):**  Space Invaders can be framed as an MDP. The current game state gives me (the AI) all the information needed to make an optimal decision.  Future states depend only on the current state and my action.\n",
      "*   **Relatively Simple Environment:**  While challenging for humans, the rules of Space Invaders are fixed and relatively straightforward.  This makes it easier to design a suitable reward function and train a Q-Learning agent.\n",
      "*   **Old School Feel:** Q-Learning is a classic reinforcement learning algorithm that fits the \"old school\" vibe of the game. It's not the bleeding edge, deep learning approach, but it's powerful enough for the task.\n",
      "\n",
      "**How I'd Implement It (The \"Old School\" ML Approach):**\n",
      "\n",
      "1.  **State Representation:** This is crucial.  I can't feed the *entire* screen into a Q-Table.  That would lead to a state space too large to manage.  Instead, I'd use a *feature-based* representation.  I'd hand-engineer features that capture important aspects of the game state. Examples:\n",
      "\n",
      "    *   **Invader Positions:**  The x, y coordinates of the closest invader (horizontally and vertically) to the player. Potentially also the number of invaders in each column.\n",
      "    *   **Player Position:**  The x coordinate of the player.\n",
      "    *   **Closest Invader's Bullet:**  Distance to the closest enemy projectile. Indicate of which bullet is below the player.\n",
      "    *   **Shield Health:**  Health remaining in the shields (perhaps represented as a count of intact shield blocks).\n",
      "    *   **Invader Speed:** The current speed of the invader formation.\n",
      "    *   **Player Bullets:** The existence and position of any player bullets in the game.\n",
      "    These features would be carefully chosen and designed to be robust and representative of the game's dynamics.\n",
      "2.  **Q-Table:** I'd initialize a Q-Table with all possible state-action pairs.  The size would depend on the number of possible values for each state feature. This is where I'd need to make choices to balance accuracy with memory usage.\n",
      "3.  **Reward Function:**  Careful design is critical.\n",
      "\n",
      "    *   **Positive Rewards:**\n",
      "        *   +1 for hitting an invader.\n",
      "        *   +0.1 for dodging a projectile.\n",
      "        *   +10 (or more) for clearing a wave.\n",
      "    *   **Negative Rewards:**\n",
      "        *   -10 for getting hit by an invader's bullet (game over).\n",
      "        *   -0.01 for doing nothing (to encourage action).  Potentially, larger negative reward for staying still under a bullet.\n",
      "    *   **Zero Reward:**  For all other actions.\n",
      "4.  **Q-Learning Update Rule:**  The standard Q-Learning update:\n",
      "\n",
      "    ```\n",
      "    Q(s, a) = Q(s, a) + α [R(s, a) + γ max_a' Q(s', a') - Q(s, a)]\n",
      "    ```\n",
      "\n",
      "    *   `Q(s, a)`: The Q-value for state `s` and action `a`.\n",
      "    *   `α`: Learning rate (how much to update the Q-value based on the new information). A value such as 0.1 would be suitable.\n",
      "    *   `R(s, a)`: The reward received for taking action `a` in state `s`.\n",
      "    *   `γ`: Discount factor (how much to value future rewards). A value such as 0.9 would be suitable.\n",
      "    *   `s'`: The next state after taking action `a` in state `s`.\n",
      "    *   `max_a' Q(s', a')`: The maximum Q-value for any action `a'` in the next state `s'`.\n",
      "5.  **Exploration vs. Exploitation:**  Use an epsilon-greedy strategy.\n",
      "\n",
      "    *   With probability `ε` (e.g., 0.1), choose a random action (exploration).  This helps the agent discover new and potentially better strategies.\n",
      "    *   With probability `1 - ε`, choose the action with the highest Q-value (exploitation).  This allows the agent to take advantage of what it has already learned.\n",
      "    *   Decay `ε` over time.  Start with a higher `ε` for more exploration, and gradually decrease it as the agent learns, favoring exploitation.\n",
      "6.  **Training:**  Run the agent through many episodes of Space Invaders.  Each episode is a single game.  Update the Q-Table after each action.\n",
      "7.  **Fine-Tuning:**  Experiment with different values for the learning rate, discount factor, and exploration rate.  The key is to find a balance between exploration and exploitation.  Also, iterate on the state representation and reward function.\n",
      "8.  **Addressing Scaling:** The number of possible states could still grow very quickly. One approach, if the Q-Table gets too large, is to use a **function approximator**, such as a simple **linear model** or a **neural network with a single hidden layer**, to estimate the Q-values.  This would involve training the model to predict Q-values based on the state features and actions.\n",
      "9. **Human Oversight:** This is the old-school approach. Keep a close eye on the agent's behavior during training. If it gets stuck in a suboptimal strategy, adjust the reward function or state representation.\n",
      "\n",
      "**Why Not Deep Learning?**\n",
      "\n",
      "While a deep reinforcement learning approach (like DQN - Deep Q-Network) *could* be used, it feels less in the spirit of \"old school.\"  DQN would require a much more complex setup, more computational power, and a lot more data. It's overkill for Space Invaders, especially if you're going for a more resource-constrained, retro approach. Hand-engineered features and a well-tuned Q-Learning implementation would be more efficient, understandable, and in line with the spirit of the question. Plus, a deep learning model, while potentially better, would be harder to debug and interpret.\n",
      "\n",
      "**In summary:** I'd use Space Invaders, Q-Learning with feature engineering, a careful reward function, and lots of training to create a formidable, \"old school\" AI invader slayer!\n",
      "\n",
      "Tokens used: 1542\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"If you were old school, which videogame would you play? What machine learning model would you use to beat the game?\"\n",
    "\n",
    "answer, usage = gemini_api_text_inquiry(prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a4c87b-09dd-49d5-8407-071ca5e3cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_api_visual_inquiry(image_path, prompt_text, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    image = PIL.Image.open(image_path)\n",
    "\n",
    "    #Choose a Gemini model.\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "\n",
    "    response = model.generate_content([prompt_text, image])\n",
    "\n",
    "    answer = response.text\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316c1b33-d955-4546-89bd-b95071777cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lymphocyte\n",
      "\n",
      "Tokens used: 378\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "prompt_text = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "answer, usage = gemini_api_visual_inquiry(image_path, prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54beb89-2293-4564-b0b3-fe3f94274e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_multiimage_api_visual_inquiry(image_paths, prompt_texts, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    if len(image_paths) != len(prompt_texts):\n",
    "        raise ValueError(\"The number of image paths and prompt texts must be the same.\")\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for image_path, prompt_text in zip(image_paths, prompt_texts):  \n",
    "        image = PIL.Image.open(image_path)\n",
    "\n",
    "        messages.append(image)\n",
    "        messages.append(prompt_text)\n",
    "        \n",
    "    #Choose a Gemini model.\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "\n",
    "    response = model.generate_content(messages)\n",
    "\n",
    "    answer = response.text\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781827f5-b9e8-4b85-af5a-f046424d30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lymphocyte\n",
      "Tokens used: 1179\n"
     ]
    }
   ],
   "source": [
    "# Train:\n",
    "train_path_0 = os.path.join(trainset_folder, \"image_0.jpg\") # It's a Myelocyte\n",
    "train_prompt_0 = \"The cell type in this image is Myelocyte\"\n",
    "\n",
    "train_path_1 = os.path.join(trainset_folder, \"image_2.jpg\") # It's a Platelet\n",
    "train_prompt_1 = \"The cell type in this image is Platelet\"\n",
    "\n",
    "train_path_2 = os.path.join(trainset_folder, \"image_9.jpg\") # It's a Lymphocyte\n",
    "train_prompt_2 = \"The cell type in this image is Lymphocyte\"\n",
    "\n",
    "# Test\n",
    "test_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "test_prompt = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "image_paths = [train_path_0, train_path_1, train_path_2, test_path]\n",
    "prompt_texts = [train_prompt_0, train_prompt_1, train_prompt_2, test_prompt]\n",
    "\n",
    "answer, usage = gemini_multiimage_api_visual_inquiry(image_paths, prompt_texts)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb160ad7-e3e4-43b3-933e-371eede23167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
