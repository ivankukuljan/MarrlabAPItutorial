{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5e9f2b-0be9-444d-9ef0-1d037f9f0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/envs/vlm_eval/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af23c368-2f8d-4f9b-97da-5e6cca2e4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root_folder = \"/home/ivan/Helmholtz/Conferences/APIworkshopHelmholtz2025/\"\n",
    "dataset_folder = os.path.join(project_root_folder, \"AcevedoDataSet\")\n",
    "testset_folder =  os.path.join(dataset_folder, \"test\")\n",
    "trainset_folder = os.path.join(dataset_folder, \"train\")\n",
    "valset_folder = os.path.join(dataset_folder, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff88037b-aa73-4aad-85cc-98d158829143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_api_text_inquiry(prompt_text, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "    \n",
    "    response = model.generate_content(prompt_text)\n",
    "\n",
    "    answer = response.text\n",
    "\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc6b2a6-3b15-4041-a844-a5200057d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Okay, if I were going old school with a game and then using a machine learning model to conquer it, here's what I'd do:\n",
      "\n",
      "**The Game:** **Ms. Pac-Man (Atari 2600 Version)**\n",
      "\n",
      "*   **Why Ms. Pac-Man (Atari 2600)?**\n",
      "    *   **Simple Rules, Complex Strategies:** The core gameplay is simple to understand (eat pellets, avoid ghosts), but mastering it requires intricate path planning and prediction of ghost behavior.\n",
      "    *   **Limited State Space:**  Compared to modern games, the game state is relatively small (screen size, ghost positions, pellet positions, etc.), making it more amenable to training a machine learning model with reasonable resources.\n",
      "    *   **Classic Arcade AI Challenge:** The original ghost AI, while iconic, is predictable and exploitable. A machine learning model can learn these patterns and surpass human performance.\n",
      "    *   **Nostalgia!** Let's be honest, it's a classic.\n",
      "\n",
      "**The Machine Learning Model:** **Deep Q-Network (DQN)**\n",
      "\n",
      "*   **Why DQN?**\n",
      "    *   **Reinforcement Learning:** Ms. Pac-Man is a perfect fit for Reinforcement Learning (RL). The model learns by interacting with the environment (the game), receiving rewards (points for eating pellets, power pellets, ghosts) and penalties (getting caught).\n",
      "    *   **Value-Based Approach:** DQN learns to estimate the Q-value, which represents the expected cumulative reward of taking a specific action in a given state.\n",
      "    *   **Deep Neural Network:** Using a deep neural network (CNN or similar) to approximate the Q-function allows the model to generalize from raw pixel input (the game screen) to learn complex patterns.\n",
      "    *   **Off-Policy Learning:** DQNs can learn from past experiences (using experience replay), which helps stabilize training and break correlations between consecutive samples.\n",
      "    *   **Suitable for Discrete Action Spaces:** Ms. Pac-Man has a small, discrete action space (up, down, left, right, no movement).\n",
      "\n",
      "**How the DQN would work:**\n",
      "\n",
      "1.  **Input:** The DQN takes the current frame from the Atari 2600 screen as input.  (Potentially pre-processed to simplify the representation, e.g., grayscale, downscaling, and stacked frames to provide motion information).\n",
      "2.  **Neural Network:** This input is fed into a convolutional neural network (CNN).  The CNN extracts features from the visual input.\n",
      "3.  **Output:** The CNN outputs a Q-value for each possible action (up, down, left, right, no movement).  The Q-value represents the estimated long-term reward for taking that action in the current state.\n",
      "4.  **Action Selection:**\n",
      "    *   During training, the agent uses an \"epsilon-greedy\" strategy. With probability `epsilon` (which decays over time), it takes a random action (exploration). With probability `1 - epsilon`, it takes the action with the highest Q-value (exploitation).\n",
      "5.  **Reward:** The agent receives a reward based on its action in the game:\n",
      "    *   +1 for eating a pellet.\n",
      "    *   +5 for eating a power pellet.\n",
      "    *   +100 to +1600 for eating a ghost after eating a power pellet (score increases with each subsequent ghost eaten)\n",
      "    *   -10 for getting caught by a ghost (end of game).\n",
      "6.  **Experience Replay:** The agent stores its experiences (state, action, reward, next state) in a replay buffer.\n",
      "7.  **Training:**  Periodically, the DQN samples a batch of experiences from the replay buffer.  It uses these experiences to update the weights of the neural network.  The target Q-values are calculated using the Bellman equation:\n",
      "\n",
      "    `Q(s, a) = r + gamma * max_a' Q(s', a')`\n",
      "\n",
      "    Where:\n",
      "    *   `Q(s, a)` is the Q-value for state `s` and action `a`.\n",
      "    *   `r` is the reward received.\n",
      "    *   `gamma` is a discount factor (between 0 and 1) that determines the importance of future rewards.\n",
      "    *   `s'` is the next state.\n",
      "    *   `max_a' Q(s', a')` is the maximum Q-value for any action in the next state.\n",
      "8.  **Iteration:** Steps 3-7 are repeated for many episodes (games). Over time, the DQN learns to predict the Q-values accurately, and it learns to play Ms. Pac-Man effectively.\n",
      "\n",
      "**Potential Challenges:**\n",
      "\n",
      "*   **Sparse Rewards:** The agent gets relatively few rewards in a typical game episode, making learning slower.  Reward shaping (e.g., giving small rewards for moving closer to pellets) could help.\n",
      "*   **Exploration:** Encouraging the agent to explore the game environment is crucial to discovering good strategies.  Careful tuning of the epsilon-greedy strategy is important.\n",
      "*   **Hyperparameter Tuning:** The performance of the DQN is sensitive to hyperparameters (learning rate, discount factor, batch size, neural network architecture, etc.).  Experimentation is needed to find the optimal values.\n",
      "*   **Computation:** Training a DQN can be computationally expensive, especially for high-resolution inputs.\n",
      "\n",
      "**Why this would be cool:**\n",
      "\n",
      "*   It would be a fun challenge to see if a machine learning model can outperform even expert human players on this classic game.\n",
      "*   It demonstrates the power of reinforcement learning to solve complex control problems.\n",
      "*   It could provide insights into the strengths and weaknesses of different machine learning approaches.\n",
      "\n",
      "In short, conquering Ms. Pac-Man (Atari 2600) with a DQN would be a satisfying blend of old-school gaming nostalgia and modern machine learning prowess!\n",
      "\n",
      "Tokens used: 1293\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"If you were old school, which videogame would you play? What machine learning model would you use to beat the game?\"\n",
    "\n",
    "answer, usage = gemini_api_text_inquiry(prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a4c87b-09dd-49d5-8407-071ca5e3cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_api_visual_inquiry(image_path, prompt_text, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    image = PIL.Image.open(image_path)\n",
    "\n",
    "    #Choose a Gemini model.\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "\n",
    "    response = model.generate_content([prompt_text, image])\n",
    "\n",
    "    answer = response.text\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316c1b33-d955-4546-89bd-b95071777cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lymphocyte\n",
      "\n",
      "Tokens used: 378\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "prompt_text = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "answer, usage = gemini_api_visual_inquiry(image_path, prompt_text)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d54beb89-2293-4564-b0b3-fe3f94274e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_multiimage_api_visual_inquiry(image_paths, prompt_texts, vlm_name='gemini-2.0-flash-exp', **kwargs): #'gemini-1.5-pro'\n",
    "\n",
    "    if len(image_paths) != len(prompt_texts):\n",
    "        raise ValueError(\"The number of image paths and prompt texts must be the same.\")\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    for image_path, prompt_text in zip(image_paths, prompt_texts):  \n",
    "        image = PIL.Image.open(image_path)\n",
    "\n",
    "        messages.append(image)\n",
    "        messages.append(prompt_text)\n",
    "        \n",
    "    #Choose a Gemini model.\n",
    "    model = genai.GenerativeModel(model_name=vlm_name)\n",
    "\n",
    "    response = model.generate_content(messages)\n",
    "\n",
    "    answer = response.text\n",
    "    usage = response.usage_metadata.total_token_count\n",
    "    \n",
    "    return answer, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "781827f5-b9e8-4b85-af5a-f046424d30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Lymphocyte\n",
      "Tokens used: 1179\n"
     ]
    }
   ],
   "source": [
    "# Train:\n",
    "train_path_0 = os.path.join(trainset_folder, \"image_0.jpg\") # It's a Myelocyte\n",
    "train_prompt_0 = \"The cell type in this image is Myelocyte\"\n",
    "\n",
    "train_path_1 = os.path.join(trainset_folder, \"image_2.jpg\") # It's a Platelet\n",
    "train_prompt_1 = \"The cell type in this image is Platelet\"\n",
    "\n",
    "train_path_2 = os.path.join(trainset_folder, \"image_9.jpg\") # It's a Lymphocyte\n",
    "train_prompt_2 = \"The cell type in this image is Lymphocyte\"\n",
    "\n",
    "# Test\n",
    "test_path = os.path.join(testset_folder, \"image_38.jpg\") # It's a Platelet\n",
    "test_prompt = \"\"\"Consider the input image. Take a moment to think. Consider what features do the cells in the image have. Which of the white blood cell types listed below is shown? \n",
    "    Write just the cell type and nothing else. Choose one of the possible labels provided below (exactly as written here):\n",
    "    Band Neutrophil\n",
    "    Basophil\n",
    "    Eosinophil\n",
    "    Erythroblast\n",
    "    Lymphocyte\n",
    "    Metamyelocyte\n",
    "    Monocyte\n",
    "    Myelocyte\n",
    "    Platelet\n",
    "    Promyelocyte\n",
    "    Segmented Neutrophil\"\"\"\n",
    "\n",
    "image_paths = [train_path_0, train_path_1, train_path_2, test_path]\n",
    "prompt_texts = [train_prompt_0, train_prompt_1, train_prompt_2, test_prompt]\n",
    "\n",
    "answer, usage = gemini_multiimage_api_visual_inquiry(image_paths, prompt_texts)\n",
    "\n",
    "print(\"Answer: \" + answer)\n",
    "print(\"Tokens used: \" + str(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb160ad7-e3e4-43b3-933e-371eede23167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
